In this chapter we talk about the state of the art in computer vision and how it has been used for remote sensing problems. We also give a brief account of natural disaster assesment, and how are these machine techniques applied in this sense. We use Sandy Huricane as a study case because of the data that was publicly made available by the NOAA.\\

\section{Computer vision}

 Le Cun \textit{et al.} \cite{lecun} propose to use an architecture of a multi-layer neural network that was able to learn directly from the data with no prior feature extraction. In contrast to the usual path that was used in the context of pattern classification, they created an architecture that was able to automatically extract the features directly from the date without prior manipulation. Instead of using a fully connected network, they proposed a locally connected net. It was capable of extracting local features and passed them down to the subsequent layers in what they called a \textit{feature map}. Each unit took the information of a $5\times 5$ neighborhood of the pixel in the previous layer. The last layer of the architecture consisted of ten units that represented each of the possible digits. This architecture was trained using backpropagation are now known as Convolutional Neural Networks (CNNs). The big leap forward of their result was that their architecture needed very little information about the task it was performing, they where able to extend the use of their method to other symbols, however, they state that the method was not able to be applied to very complex objects. \\


 With the tremendous advances that computer power has suffer in the late years, this has been proven to be incorrect. In 2009 a big image database was gather and published \cite{imagenet}. Ever since this database became the defacto dataset to test classification methods. A few years later, in 2012 Krizhevsky \textit{et al.} \cite{krizhevsky} proposed the use of CNNs in this daunting task.

\section{Remote sensing}


In late years groundbreaking advances in computer vision have had a tremendous impact in other science fields. In particular, we are interested in landcover classification.\\


The use of CNNs in the context of landcover classification was explored by Kussul \textit{et al.} \cite{kussul}. They used an ensamble of CNNs to obtain state of the art results in the classification of different types of crops using multitemporal and multisensor satellite data. They explore 2 aproaches, first they use a 1-D CNN to perform the convolutions in the spectral domain by stacking the different bands from the Sentinel-1 A and La ndsat-8 scenes. This process outputs a pixelwise classification, then they perform a traditional 2-D CNN on the scenes. In order not to lose resolution with the 2-D CNN, they use a sliding window approach assigning the class to the center pixel of the sliding window. Finally, they ensamble both opinions and filter the result to improve the quality of the map.\\

The usual approach with landcover classification is the use of classical classification methods such as support vector machines (SVM) and random forests (RF). In order to improve the performance, features must be handcrafted from the original bands. In \cite{scott}, Grant \textit{et al.} explore the use of Transfer Learning and Data Augmentation in the context of remote sensing images. By exploring well-known high-resolution datasets, they obtain state of the art results.\\

Transfer Learning (TF) is the process of using an already trained CNN, to 

\subsection{Data augmentation}

Data augmentation is a tecnique used to artificially increment the size of the training dataset by appliying affine transformation to the images. It is often used when tagged data is scarce and difficult to obtain. The usual transformations include rotations and relections. When using this tecnique we should be careful about the orientation of the objects, for example a building upside down makes no sense, so there is no use to make the network learn features on objects that it won't see in the wild. Fortunately, aerial imagery don't present this problem. There is no particular orientation that can be considered correct when the pictures are taken from above. This means that we can dramatically boost the size of our dataset.\\

The reasoning behind this idea is that when we see a picture, our brain autmatically orients it into its correct position. By showing the network with different positions and orientations of an object we enrich its knowledge about it.

We can think of the neural network as a newborn kid, in the begining it experiences its environment for the first time



\section{Damage Assessment}


