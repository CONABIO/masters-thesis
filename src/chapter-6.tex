


We built a data pipeline from end to end. This exercise was enlightening and showed many of the issues that can appear when we take such endeavor. In this chapter, we review the ideas that we did not inspect deeper, and some obstacles found in the process.\\

\section{Data Science}

The product that we developed during this research might serve as a baseline in the event of a new natural disaster. The process involved the use of several techniques and tools to create a data pipeline capable of detecting damaged buildings automatically using drone imagery. The purpose was to provide the decision makers with a tool that allow them to allocate resources efficiently. We understand the inherent limitations of automatic methods, but we believe that we can achieve greater things working together with this new tools. Machine learning methods are not a panacea, but a useful device that can help us reach places we could just imagine before.\\

The discussion of the need for a correct use of the information extracted with our system was an additional topic that was not covered. Maps are only useful when they serve as tools to make better decisions, otherwise, they end up being worthless drawings hanging on the wall. Operations research is the discipline that can help our idea to have a real impact. For example, we envision a situation in which our map can lead to a correct allocation of people to go on site to inspect the buildings and a dispatch system that makes every second count. Another imaginable scenario would be to compare the model's predictions with the official information to spot anomalies. This would bring light to processes that are liable to be tainted by corruption. Leveraging the advances in data processing and the many sources available is key to a better future.\\

\section{Drawbacks}

We noticed that, even when the classifier performs well in environments different to the one we trained it with, it learns to classify debris, not precisely damaged buildings. We noticed some examples in which the classifier correctly finds scenes with the presence of debris, however, when we inspected the place using Google Street View, we saw that there was no building in that area. This evidence can point to two possible scenarios; there was never a building in that place, and the site was used as a disposal from other areas, or the house was not there when Google Street View took the picture. Both cases show inherent limitations of the methodology that we are proposing; we can only automate this kind of process to a certain extent.\\

One aspect of the proceeding that we did not include in this research was the postprocessing of drone data. CENAPRED gave us a post-processed raster with the georectification already applied. This process requires specialized software that reads the images and geolocates ground control points that act as anchors for the pictures to be geolocated. This technique requires human experts to aid the software by manually inspecting the limitations of the automatized process.\\

Another limitation found in the process was the lack of reliable training data. This obstacle was the reason that the author needed to tag the images. In the future, we expect to have curated data from previous events to make the process even faster. As of today, our work may serve as a base for future improvements that allow this tool to get into production.\\

We should also take ethical considerations into account. Should the information that the system outputs be public? What if people use it for nefarious purposes such as looting? With the power of data follows a greater responsibility.\\

\section{Future work}

We divided this section in two: first, we mention which features can be improved and incorporated into our pipeline, and then we image how these ideas can impact other fields of interest. By no means, this review pretends to be exhaustive, but just to give a broader picture.\\

\subsection{Areas of opportunity}

An idea that was beyond the scope of this research was to incorporate a technique known as active learning. In this scheme, the algorithm keeps improving as it receives feedback from the experts. In this fashion, when the model makes mistakes, this incorrectly labeled scenes can be relabeled and input back into the system to create a new model with improved performance. Although there is a limit to the extent in which the algorithm can perform, this might aid in some prominent cases that might not be present in the original training data.\\

We used the Inception model because it gave us the scaffolding to produce an application minimizing the development efforts. It was the principal objective to test the ability of trained CNN models to be used in other tasks that the ones for what they were built. However, it would be interesting to create a model from the ground and fine tune it for our specific task. Inception is a complex model because of its nature; its classification capabilities lie far beyond the simple job for what we are using it for. This means that the resources it takes could be dramatically downsized. A tailor fit model would be more performant, but in the other end, it would also need vast amounts of training data.\\

The tool does not consider the different use cases that can come up in its real environment. Site verifiers would need to have access to the interface that allows to tag new training data in the same way that the taggers would need no access to administrative duties such as user creation and full access to the REST API.\\

\subsection{Remote sensing}


Another aspect that would be interesting to explore is the application of the same analysis framework in another type of studies. In the National Commission for the Knowledge and Use of Biodiversity (CONABIO), we use landcover maps to analyze and assess the evolution of the environment through time. We make this possible by leveraging standard classification algorithms and a significant amount of computing power. While our efforts have been quite productive, these algorithms have certain limits; they rely on the use of the light spectrum. As a consequence, any two categories with similar spectral signature will, in all likelihood, confuse the classifier.\\

For example, crops and grasslands might seem identical to a supervised classifier. Curiously enough, humans have little problem distinguish between such a pair of categories. The human eye can spot the difference from one another. This behavior is because we are not seeing particular pixels and trying to classify them one by one. Instead, our brain takes a look at the whole picture, and we focus on segments of the image that contain most of the information, in other words, we care about context. CNNs take this into account. Each neuron of the network receives only a zone of the image when information flow through the layers of the system, individual neurons activate upon specific stimuli. This process allows the network to recognize some features that would be invisible to a standard classifier. We can think of this as if the system engineers its features on the fly.\\

Efforts at CONABIO focus to attain a particular objective; to build a comprehensive biodiversity monitoring system. It can be though as two independent attempts. One is the Monitoring Activity Data for the Mexican REDD+ program (MADMex) \cite{rs6053923} which pretends to monitor the behavior of forest and vegetation across the country by processing satellite imagery. The other is the Mexican National Biodiversity and Ecosystem Degradation Monitoring System (SNMB) \cite{GARCIAALANIZ201762} which gathers information about species in the different ecosystems that exist in Mexico. Both projects can benefit from this novel techniques.\\

\section{Conclusion}

Our efforts showed that it is possible to deliver a preliminary product with little training effort. The performance that a fully trained CNN reaches is outstanding compared to traditional methods. The accessibility of the pieces needed to build such a system makes it possible to create a system that helps in the aftermath of natural resources with little investment.\\

This subject is prominent nowadays, and the time has come to create tools that make our lives better. The objective of this work was to bring an efficient tool targeting the difficulties found in the real world. We thought of a device that allowed fast and efficient allocation of resources. Aiming places in which infrastructure is not as advanced as in big cities where we can employ other methods.\\

Recent events taught us essential lessons about what to do and what not to do in the aftermath. Earthquakes will continue to shake our realities. Our assignment is to prepare our society to confront these events most resiliently.\\
